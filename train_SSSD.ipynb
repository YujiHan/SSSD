{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension for cauchy multiplication not found. Install by going to extensions/cauchy/ and running `python setup.py install`. This should speed up end-to-end training by 10-50%\n",
      "Falling back on slow Cauchy kernel. Install at least one of pykeops or the CUDA extension for efficiency.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.util import print_size, training_loss, calc_diffusion_hyperparams\n",
    "from utils.util import get_mask_mnr, get_mask_bm, get_mask_rm\n",
    "\n",
    "from imputers.SSSDS4Imputer import SSSDS4Imputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global trainset_config\n",
    "global diffusion_hyperparams\n",
    "global model_config\n",
    "\n",
    "config = {   \n",
    "    \"diffusion_config\":{\n",
    "        \"T\": 200,\n",
    "        \"beta_0\": 0.0001,\n",
    "        \"beta_T\": 0.02\n",
    "    },\n",
    "    \"wavenet_config\": {\n",
    "        \"in_channels\": 14, \n",
    "        \"out_channels\":14,\n",
    "        \"num_res_layers\": 36,\n",
    "        \"res_channels\": 256, \n",
    "        \"skip_channels\": 256,\n",
    "        \"diffusion_step_embed_dim_in\": 128,\n",
    "        \"diffusion_step_embed_dim_mid\": 512,\n",
    "        \"diffusion_step_embed_dim_out\": 512,\n",
    "        \"s4_lmax\": 100,\n",
    "        \"s4_d_state\":64,\n",
    "        \"s4_dropout\":0.0,\n",
    "        \"s4_bidirectional\":1,\n",
    "        \"s4_layernorm\":1\n",
    "    },\n",
    "    \"train_config\": {\n",
    "        \"output_directory\": \"./results/mujoco\",\n",
    "        \"ckpt_iter\": -1,\n",
    "        \"iters_per_ckpt\": 100,\n",
    "        \"iters_per_logging\": 100,\n",
    "        \"n_iters\": 150000,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"only_generate_missing\": 1,\n",
    "        \"use_model\": 2,\n",
    "        \"masking\": \"rm\",\n",
    "        \"missing_k\": 90\n",
    "    },\n",
    "    \"trainset_config\":{\n",
    "        \"train_data_path\": \"/home/hanyuji/data/mujoco_dataset/train_mujoco.npy\",\n",
    "        \"test_data_path\": \"/home/hanyuji/data/mujoco_dataset/test_mujoco.npy\",\n",
    "        \"segment_length\":100,\n",
    "        \"sampling_rate\": 100\n",
    "    },\n",
    "    \"gen_config\":{\n",
    "        \"output_directory\": \"./results/mujoco\",\n",
    "        \"ckpt_path\": \"./results/mujoco/\"\n",
    "    }\n",
    "}\n",
    "\n",
    "train_config = config[\"train_config\"]  # training parameters\n",
    "trainset_config = config[\"trainset_config\"]  # to load trainset\n",
    "model_config = config['wavenet_config']\n",
    "diffusion_hyperparams = calc_diffusion_hyperparams(**config[\"diffusion_config\"])  # dictionary of all diffusion hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTrain Diffusion Models\\n\\nParameters:\\noutput_directory (str):         save model checkpoints to this path\\nckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded; \\n                                automatically selects the maximum iteration if 'max' is selected\\ndata_path (str):                path to dataset, numpy array.\\nn_iters (int):                  number of iterations to train\\niters_per_ckpt (int):           number of iterations to save checkpoint, \\n                                default is 10k, for models with residual_channel=64 this number can be larger\\niters_per_logging (int):        number of iterations to save training log and compute validation loss, default is 100\\nlearning_rate (float):          learning rate\\n\\nuse_model (int):                0:DiffWave. 1:SSSDSA. 2:SSSDS4.\\nonly_generate_missing (int):    0:all sample diffusion.  1:only apply diffusion to missing portions of the signal\\nmasking(str):                   'mnr': missing not at random, 'bm': blackout missing, 'rm': random missing\\nmissing_k (int):                k missing time steps for each feature across the sample length.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train(**train_config)\n",
    "\n",
    "output_directory = train_config['output_directory']\n",
    "ckpt_iter = train_config['ckpt_iter']\n",
    "iters_per_ckpt = train_config['iters_per_ckpt']\n",
    "iters_per_logging = train_config['iters_per_logging']\n",
    "n_iters = train_config['n_iters']\n",
    "learning_rate = train_config['learning_rate']\n",
    "only_generate_missing = train_config['only_generate_missing']\n",
    "masking = train_config['masking']\n",
    "missing_k = train_config['missing_k']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train Diffusion Models\n",
    "\n",
    "Parameters:\n",
    "output_directory (str):         save model checkpoints to this path\n",
    "ckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded; \n",
    "                                automatically selects the maximum iteration if 'max' is selected\n",
    "data_path (str):                path to dataset, numpy array.\n",
    "n_iters (int):                  number of iterations to train\n",
    "iters_per_ckpt (int):           number of iterations to save checkpoint, \n",
    "                                default is 10k, for models with residual_channel=64 this number can be larger\n",
    "iters_per_logging (int):        number of iterations to save training log and compute validation loss, default is 100\n",
    "learning_rate (float):          learning rate\n",
    "\n",
    "use_model (int):                0:DiffWave. 1:SSSDSA. 2:SSSDS4.\n",
    "only_generate_missing (int):    0:all sample diffusion.  1:only apply diffusion to missing portions of the signal\n",
    "masking(str):                   'mnr': missing not at random, 'bm': blackout missing, 'rm': random missing\n",
    "missing_k (int):                k missing time steps for each feature across the sample length.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSSDS4Imputer Parameters: 48.371726M\n"
     ]
    }
   ],
   "source": [
    "# map diffusion hyperparameters to gpu\n",
    "for key in diffusion_hyperparams:\n",
    "    if key != \"T\":\n",
    "        diffusion_hyperparams[key] = diffusion_hyperparams[key].cuda()\n",
    "\n",
    "# predefine model\n",
    "net = SSSDS4Imputer(**model_config).cuda()\n",
    "print_size(net)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 100, 14)\n",
      "(160, 50, 100, 14)\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "### data loading and reshaping ###\n",
    "\n",
    "training_data = np.load(trainset_config['train_data_path'])\n",
    "print(training_data.shape)\n",
    "training_data = np.split(training_data, 160, 0)\n",
    "training_data = np.array(training_data)\n",
    "print(training_data.shape)\n",
    "\n",
    "training_data = torch.from_numpy(training_data).float().cuda()\n",
    "print('Data loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 \tloss: 0.9102503061294556\n",
      "iteration: 100 \tloss: 0.48290690779685974\n",
      "model at iteration 100 is saved\n",
      "iteration: 200 \tloss: 0.2387528270483017\n",
      "model at iteration 200 is saved\n",
      "iteration: 300 \tloss: 0.11651907861232758\n",
      "model at iteration 300 is saved\n",
      "iteration: 400 \tloss: 0.05717376619577408\n",
      "model at iteration 400 is saved\n",
      "iteration: 500 \tloss: 0.04107207432389259\n",
      "model at iteration 500 is saved\n",
      "iteration: 600 \tloss: 0.03194050118327141\n",
      "model at iteration 600 is saved\n",
      "iteration: 700 \tloss: 0.028316810727119446\n",
      "model at iteration 700 is saved\n",
      "iteration: 800 \tloss: 0.046355199068784714\n",
      "model at iteration 800 is saved\n",
      "iteration: 900 \tloss: 0.02934068627655506\n",
      "model at iteration 900 is saved\n",
      "iteration: 1000 \tloss: 0.026989130303263664\n",
      "model at iteration 1000 is saved\n",
      "iteration: 1100 \tloss: 0.022299429401755333\n",
      "model at iteration 1100 is saved\n",
      "iteration: 1200 \tloss: 0.021887024864554405\n",
      "model at iteration 1200 is saved\n",
      "iteration: 1300 \tloss: 0.02610797993838787\n",
      "model at iteration 1300 is saved\n",
      "iteration: 1400 \tloss: 0.030341938138008118\n",
      "model at iteration 1400 is saved\n",
      "iteration: 1500 \tloss: 0.028145864605903625\n",
      "model at iteration 1500 is saved\n",
      "iteration: 1600 \tloss: 0.024557439610362053\n",
      "model at iteration 1600 is saved\n",
      "iteration: 1700 \tloss: 0.025595899671316147\n",
      "model at iteration 1700 is saved\n",
      "iteration: 1800 \tloss: 0.026730284094810486\n",
      "model at iteration 1800 is saved\n",
      "iteration: 1900 \tloss: 0.02166418544948101\n",
      "model at iteration 1900 is saved\n",
      "iteration: 2000 \tloss: 0.021491587162017822\n",
      "model at iteration 2000 is saved\n",
      "iteration: 2100 \tloss: 0.01945306360721588\n",
      "model at iteration 2100 is saved\n",
      "iteration: 2200 \tloss: 0.014283251948654652\n",
      "model at iteration 2200 is saved\n",
      "iteration: 2300 \tloss: 0.02216174826025963\n",
      "model at iteration 2300 is saved\n",
      "iteration: 2400 \tloss: 0.023265110328793526\n",
      "model at iteration 2400 is saved\n",
      "iteration: 2500 \tloss: 0.01775696873664856\n",
      "model at iteration 2500 is saved\n",
      "iteration: 2600 \tloss: 0.018296658992767334\n",
      "model at iteration 2600 is saved\n",
      "iteration: 2700 \tloss: 0.014942476525902748\n",
      "model at iteration 2700 is saved\n",
      "iteration: 2800 \tloss: 0.013846879824995995\n",
      "model at iteration 2800 is saved\n",
      "iteration: 2900 \tloss: 0.012786620296537876\n",
      "model at iteration 2900 is saved\n",
      "iteration: 3000 \tloss: 0.013988355174660683\n",
      "model at iteration 3000 is saved\n",
      "iteration: 3100 \tloss: 0.022820960730314255\n",
      "model at iteration 3100 is saved\n",
      "iteration: 3200 \tloss: 0.0151210380718112\n",
      "model at iteration 3200 is saved\n",
      "iteration: 3300 \tloss: 0.017670663073658943\n",
      "model at iteration 3300 is saved\n",
      "iteration: 3400 \tloss: 0.015148989856243134\n",
      "model at iteration 3400 is saved\n",
      "iteration: 3500 \tloss: 0.012068504467606544\n",
      "model at iteration 3500 is saved\n",
      "iteration: 3600 \tloss: 0.017583554610610008\n",
      "model at iteration 3600 is saved\n",
      "iteration: 3700 \tloss: 0.013782600872218609\n",
      "model at iteration 3700 is saved\n",
      "iteration: 3800 \tloss: 0.009364300407469273\n",
      "model at iteration 3800 is saved\n",
      "iteration: 3900 \tloss: 0.013992409221827984\n",
      "model at iteration 3900 is saved\n",
      "iteration: 4000 \tloss: 0.014864921569824219\n",
      "model at iteration 4000 is saved\n",
      "iteration: 4100 \tloss: 0.018103813752532005\n",
      "model at iteration 4100 is saved\n",
      "iteration: 4200 \tloss: 0.01615322008728981\n",
      "model at iteration 4200 is saved\n",
      "iteration: 4300 \tloss: 0.01126728393137455\n",
      "model at iteration 4300 is saved\n",
      "iteration: 4400 \tloss: 0.009558095596730709\n",
      "model at iteration 4400 is saved\n",
      "iteration: 4500 \tloss: 0.012028000317513943\n",
      "model at iteration 4500 is saved\n",
      "iteration: 4600 \tloss: 0.014832609333097935\n",
      "model at iteration 4600 is saved\n",
      "iteration: 4700 \tloss: 0.010647451505064964\n",
      "model at iteration 4700 is saved\n",
      "iteration: 4800 \tloss: 0.009296538308262825\n",
      "model at iteration 4800 is saved\n",
      "iteration: 4900 \tloss: 0.010726112872362137\n",
      "model at iteration 4900 is saved\n",
      "iteration: 5000 \tloss: 0.00966152548789978\n",
      "model at iteration 5000 is saved\n",
      "iteration: 5100 \tloss: 0.012041863054037094\n",
      "model at iteration 5100 is saved\n",
      "iteration: 5200 \tloss: 0.015907298773527145\n",
      "model at iteration 5200 is saved\n",
      "iteration: 5300 \tloss: 0.011413580738008022\n",
      "model at iteration 5300 is saved\n",
      "iteration: 5400 \tloss: 0.008176106959581375\n",
      "model at iteration 5400 is saved\n",
      "iteration: 5500 \tloss: 0.008603882975876331\n",
      "model at iteration 5500 is saved\n",
      "iteration: 5600 \tloss: 0.013174768537282944\n",
      "model at iteration 5600 is saved\n",
      "iteration: 5700 \tloss: 0.00855040643364191\n",
      "model at iteration 5700 is saved\n",
      "iteration: 5800 \tloss: 0.00991861429065466\n",
      "model at iteration 5800 is saved\n",
      "iteration: 5900 \tloss: 0.014543609693646431\n",
      "model at iteration 5900 is saved\n",
      "iteration: 6000 \tloss: 0.011836319230496883\n",
      "model at iteration 6000 is saved\n",
      "iteration: 6100 \tloss: 0.0077353110536932945\n",
      "model at iteration 6100 is saved\n",
      "iteration: 6200 \tloss: 0.009250597096979618\n",
      "model at iteration 6200 is saved\n",
      "iteration: 6300 \tloss: 0.014457274228334427\n",
      "model at iteration 6300 is saved\n",
      "iteration: 6400 \tloss: 0.01162242703139782\n",
      "model at iteration 6400 is saved\n",
      "iteration: 6500 \tloss: 0.011021529324352741\n",
      "model at iteration 6500 is saved\n",
      "iteration: 6600 \tloss: 0.013304953463375568\n",
      "model at iteration 6600 is saved\n",
      "iteration: 6700 \tloss: 0.008105830289423466\n",
      "model at iteration 6700 is saved\n",
      "iteration: 6800 \tloss: 0.011699341237545013\n",
      "model at iteration 6800 is saved\n",
      "iteration: 6900 \tloss: 0.008823959156870842\n",
      "model at iteration 6900 is saved\n",
      "iteration: 7000 \tloss: 0.010812407359480858\n",
      "model at iteration 7000 is saved\n",
      "iteration: 7100 \tloss: 0.011654702015221119\n",
      "model at iteration 7100 is saved\n",
      "iteration: 7200 \tloss: 0.01077822782099247\n",
      "model at iteration 7200 is saved\n",
      "iteration: 7300 \tloss: 0.013652410358190536\n",
      "model at iteration 7300 is saved\n",
      "iteration: 7400 \tloss: 0.009770327247679234\n",
      "model at iteration 7400 is saved\n",
      "iteration: 7500 \tloss: 0.011154376901686192\n",
      "model at iteration 7500 is saved\n",
      "iteration: 7600 \tloss: 0.006842876318842173\n",
      "model at iteration 7600 is saved\n",
      "iteration: 7700 \tloss: 0.00951579213142395\n",
      "model at iteration 7700 is saved\n",
      "iteration: 7800 \tloss: 0.008323298767209053\n",
      "model at iteration 7800 is saved\n",
      "iteration: 7900 \tloss: 0.0060597690753638744\n",
      "model at iteration 7900 is saved\n",
      "iteration: 8000 \tloss: 0.010470482520759106\n",
      "model at iteration 8000 is saved\n",
      "iteration: 8100 \tloss: 0.006600482854992151\n",
      "model at iteration 8100 is saved\n",
      "iteration: 8200 \tloss: 0.009104684926569462\n",
      "model at iteration 8200 is saved\n",
      "iteration: 8300 \tloss: 0.01220951322466135\n",
      "model at iteration 8300 is saved\n",
      "iteration: 8400 \tloss: 0.00917786080390215\n",
      "model at iteration 8400 is saved\n",
      "iteration: 8500 \tloss: 0.007800931576639414\n",
      "model at iteration 8500 is saved\n",
      "iteration: 8600 \tloss: 0.010252236388623714\n",
      "model at iteration 8600 is saved\n",
      "iteration: 8700 \tloss: 0.008327838033437729\n",
      "model at iteration 8700 is saved\n",
      "iteration: 8800 \tloss: 0.013329417444765568\n",
      "model at iteration 8800 is saved\n",
      "iteration: 8900 \tloss: 0.010032100602984428\n",
      "model at iteration 8900 is saved\n",
      "iteration: 9000 \tloss: 0.011836602352559566\n",
      "model at iteration 9000 is saved\n",
      "iteration: 9100 \tloss: 0.010710914619266987\n",
      "model at iteration 9100 is saved\n",
      "iteration: 9200 \tloss: 0.009141799062490463\n",
      "model at iteration 9200 is saved\n",
      "iteration: 9300 \tloss: 0.006955399177968502\n",
      "model at iteration 9300 is saved\n",
      "iteration: 9400 \tloss: 0.009995202533900738\n",
      "model at iteration 9400 is saved\n",
      "iteration: 9500 \tloss: 0.01285751536488533\n",
      "model at iteration 9500 is saved\n",
      "iteration: 9600 \tloss: 0.00775464391335845\n",
      "model at iteration 9600 is saved\n",
      "iteration: 9700 \tloss: 0.0057667759247124195\n",
      "model at iteration 9700 is saved\n",
      "iteration: 9800 \tloss: 0.00607041222974658\n",
      "model at iteration 9800 is saved\n",
      "iteration: 9900 \tloss: 0.008327134884893894\n",
      "model at iteration 9900 is saved\n",
      "iteration: 10000 \tloss: 0.00939476303756237\n",
      "model at iteration 10000 is saved\n",
      "iteration: 10100 \tloss: 0.0075721279717981815\n",
      "model at iteration 10100 is saved\n",
      "iteration: 10200 \tloss: 0.006348130293190479\n",
      "model at iteration 10200 is saved\n",
      "iteration: 10300 \tloss: 0.009700176306068897\n",
      "model at iteration 10300 is saved\n",
      "iteration: 10400 \tloss: 0.0046336534433066845\n",
      "model at iteration 10400 is saved\n",
      "iteration: 10500 \tloss: 0.006561148911714554\n",
      "model at iteration 10500 is saved\n",
      "iteration: 10600 \tloss: 0.007075070403516293\n",
      "model at iteration 10600 is saved\n",
      "iteration: 10700 \tloss: 0.005810381844639778\n",
      "model at iteration 10700 is saved\n",
      "iteration: 10800 \tloss: 0.007341962773352861\n",
      "model at iteration 10800 is saved\n",
      "iteration: 10900 \tloss: 0.007804736960679293\n",
      "model at iteration 10900 is saved\n",
      "iteration: 11000 \tloss: 0.007263265550136566\n",
      "model at iteration 11000 is saved\n",
      "iteration: 11100 \tloss: 0.004082737490534782\n",
      "model at iteration 11100 is saved\n",
      "iteration: 11200 \tloss: 0.009370303712785244\n",
      "model at iteration 11200 is saved\n",
      "iteration: 11300 \tloss: 0.00784739013761282\n",
      "model at iteration 11300 is saved\n",
      "iteration: 11400 \tloss: 0.005571176763623953\n",
      "model at iteration 11400 is saved\n",
      "iteration: 11500 \tloss: 0.00765957310795784\n",
      "model at iteration 11500 is saved\n",
      "iteration: 11600 \tloss: 0.007376482710242271\n",
      "model at iteration 11600 is saved\n",
      "iteration: 11700 \tloss: 0.00565097201615572\n",
      "model at iteration 11700 is saved\n",
      "iteration: 11800 \tloss: 0.010760575532913208\n",
      "model at iteration 11800 is saved\n",
      "iteration: 11900 \tloss: 0.00596480630338192\n",
      "model at iteration 11900 is saved\n",
      "iteration: 12000 \tloss: 0.005447383038699627\n",
      "model at iteration 12000 is saved\n",
      "iteration: 12100 \tloss: 0.013225657865405083\n",
      "model at iteration 12100 is saved\n",
      "iteration: 12200 \tloss: 0.00763859273865819\n",
      "model at iteration 12200 is saved\n",
      "iteration: 12300 \tloss: 0.008180173113942146\n",
      "model at iteration 12300 is saved\n",
      "iteration: 12400 \tloss: 0.009331918321549892\n",
      "model at iteration 12400 is saved\n",
      "iteration: 12500 \tloss: 0.006686150562018156\n",
      "model at iteration 12500 is saved\n",
      "iteration: 12600 \tloss: 0.008081036619842052\n",
      "model at iteration 12600 is saved\n",
      "iteration: 12700 \tloss: 0.010906721465289593\n",
      "model at iteration 12700 is saved\n",
      "iteration: 12800 \tloss: 0.006799816619604826\n",
      "model at iteration 12800 is saved\n",
      "iteration: 12900 \tloss: 0.01203690841794014\n",
      "model at iteration 12900 is saved\n",
      "iteration: 13000 \tloss: 0.005813480820506811\n",
      "model at iteration 13000 is saved\n",
      "iteration: 13100 \tloss: 0.008205065503716469\n",
      "model at iteration 13100 is saved\n",
      "iteration: 13200 \tloss: 0.007052648346871138\n",
      "model at iteration 13200 is saved\n",
      "iteration: 13300 \tloss: 0.005078538320958614\n",
      "model at iteration 13300 is saved\n",
      "iteration: 13400 \tloss: 0.008162444457411766\n",
      "model at iteration 13400 is saved\n",
      "iteration: 13500 \tloss: 0.0054223807528615\n",
      "model at iteration 13500 is saved\n",
      "iteration: 13600 \tloss: 0.005247920285910368\n",
      "model at iteration 13600 is saved\n",
      "iteration: 13700 \tloss: 0.008852172642946243\n",
      "model at iteration 13700 is saved\n",
      "iteration: 13800 \tloss: 0.00870116613805294\n",
      "model at iteration 13800 is saved\n",
      "iteration: 13900 \tloss: 0.005629555322229862\n",
      "model at iteration 13900 is saved\n",
      "iteration: 14000 \tloss: 0.006040704902261496\n",
      "model at iteration 14000 is saved\n",
      "iteration: 14100 \tloss: 0.006160251330584288\n",
      "model at iteration 14100 is saved\n",
      "iteration: 14200 \tloss: 0.00774244824424386\n",
      "model at iteration 14200 is saved\n",
      "iteration: 14300 \tloss: 0.006514435634016991\n",
      "model at iteration 14300 is saved\n",
      "iteration: 14400 \tloss: 0.005655552260577679\n",
      "model at iteration 14400 is saved\n",
      "iteration: 14500 \tloss: 0.0054200179874897\n",
      "model at iteration 14500 is saved\n",
      "iteration: 14600 \tloss: 0.005814618431031704\n",
      "model at iteration 14600 is saved\n",
      "iteration: 14700 \tloss: 0.0059827477671206\n",
      "model at iteration 14700 is saved\n",
      "iteration: 14800 \tloss: 0.007230243179947138\n",
      "model at iteration 14800 is saved\n",
      "iteration: 14900 \tloss: 0.004631823860108852\n",
      "model at iteration 14900 is saved\n",
      "iteration: 15000 \tloss: 0.00819208100438118\n",
      "model at iteration 15000 is saved\n",
      "iteration: 15100 \tloss: 0.009099227376282215\n",
      "model at iteration 15100 is saved\n",
      "iteration: 15200 \tloss: 0.007386056240648031\n",
      "model at iteration 15200 is saved\n",
      "iteration: 15300 \tloss: 0.006233903579413891\n",
      "model at iteration 15300 is saved\n",
      "iteration: 15400 \tloss: 0.008710268884897232\n",
      "model at iteration 15400 is saved\n",
      "iteration: 15500 \tloss: 0.005637019407004118\n",
      "model at iteration 15500 is saved\n",
      "iteration: 15600 \tloss: 0.0050777411088347435\n",
      "model at iteration 15600 is saved\n",
      "iteration: 15700 \tloss: 0.0066122086718678474\n",
      "model at iteration 15700 is saved\n",
      "iteration: 15800 \tloss: 0.006614046636968851\n",
      "model at iteration 15800 is saved\n",
      "iteration: 15900 \tloss: 0.005979795474559069\n",
      "model at iteration 15900 is saved\n",
      "iteration: 16000 \tloss: 0.005303920246660709\n",
      "model at iteration 16000 is saved\n",
      "iteration: 16100 \tloss: 0.005127194337546825\n",
      "model at iteration 16100 is saved\n",
      "iteration: 16200 \tloss: 0.00454645324498415\n",
      "model at iteration 16200 is saved\n",
      "iteration: 16300 \tloss: 0.0046099708415567875\n",
      "model at iteration 16300 is saved\n",
      "iteration: 16400 \tloss: 0.006421738304197788\n",
      "model at iteration 16400 is saved\n",
      "iteration: 16500 \tloss: 0.005511057563126087\n",
      "model at iteration 16500 is saved\n",
      "iteration: 16600 \tloss: 0.005446301773190498\n",
      "model at iteration 16600 is saved\n",
      "iteration: 16700 \tloss: 0.005502159707248211\n",
      "model at iteration 16700 is saved\n",
      "iteration: 16800 \tloss: 0.0047121476382017136\n",
      "model at iteration 16800 is saved\n",
      "iteration: 16900 \tloss: 0.005690088029950857\n",
      "model at iteration 16900 is saved\n",
      "iteration: 17000 \tloss: 0.005441848188638687\n",
      "model at iteration 17000 is saved\n",
      "iteration: 17100 \tloss: 0.003948058933019638\n",
      "model at iteration 17100 is saved\n",
      "iteration: 17200 \tloss: 0.0063134776428341866\n",
      "model at iteration 17200 is saved\n",
      "iteration: 17300 \tloss: 0.0067631397396326065\n",
      "model at iteration 17300 is saved\n",
      "iteration: 17400 \tloss: 0.005857415962964296\n",
      "model at iteration 17400 is saved\n",
      "iteration: 17500 \tloss: 0.008714118972420692\n",
      "model at iteration 17500 is saved\n",
      "iteration: 17600 \tloss: 0.004862197209149599\n",
      "model at iteration 17600 is saved\n",
      "iteration: 17700 \tloss: 0.004281792789697647\n",
      "model at iteration 17700 is saved\n",
      "iteration: 17800 \tloss: 0.0035892135929316282\n",
      "model at iteration 17800 is saved\n",
      "iteration: 17900 \tloss: 0.006141473073512316\n",
      "model at iteration 17900 is saved\n",
      "iteration: 18000 \tloss: 0.00610722228884697\n",
      "model at iteration 18000 is saved\n",
      "iteration: 18100 \tloss: 0.006903341040015221\n",
      "model at iteration 18100 is saved\n",
      "iteration: 18200 \tloss: 0.005084489472210407\n",
      "model at iteration 18200 is saved\n",
      "iteration: 18300 \tloss: 0.00754367234185338\n",
      "model at iteration 18300 is saved\n",
      "iteration: 18400 \tloss: 0.004778644070029259\n",
      "model at iteration 18400 is saved\n",
      "iteration: 18500 \tloss: 0.005527166184037924\n",
      "model at iteration 18500 is saved\n",
      "iteration: 18600 \tloss: 0.003976708743721247\n",
      "model at iteration 18600 is saved\n",
      "iteration: 18700 \tloss: 0.0049344501458108425\n",
      "model at iteration 18700 is saved\n",
      "iteration: 18800 \tloss: 0.004551667720079422\n",
      "model at iteration 18800 is saved\n",
      "iteration: 18900 \tloss: 0.003488728776574135\n",
      "model at iteration 18900 is saved\n",
      "iteration: 19000 \tloss: 0.006103623192757368\n",
      "model at iteration 19000 is saved\n",
      "iteration: 19100 \tloss: 0.00594716751947999\n",
      "model at iteration 19100 is saved\n",
      "iteration: 19200 \tloss: 0.0070073422975838184\n",
      "model at iteration 19200 is saved\n",
      "iteration: 19300 \tloss: 0.005464446265250444\n",
      "model at iteration 19300 is saved\n",
      "iteration: 19400 \tloss: 0.006489122286438942\n",
      "model at iteration 19400 is saved\n",
      "iteration: 19500 \tloss: 0.005149331875145435\n",
      "model at iteration 19500 is saved\n",
      "iteration: 19600 \tloss: 0.0031451431568711996\n",
      "model at iteration 19600 is saved\n",
      "iteration: 19700 \tloss: 0.004014645703136921\n",
      "model at iteration 19700 is saved\n",
      "iteration: 19800 \tloss: 0.003955216147005558\n",
      "model at iteration 19800 is saved\n",
      "iteration: 19900 \tloss: 0.0035168705508112907\n",
      "model at iteration 19900 is saved\n",
      "iteration: 20000 \tloss: 0.003006009617820382\n",
      "model at iteration 20000 is saved\n",
      "iteration: 20100 \tloss: 0.005402659066021442\n",
      "model at iteration 20100 is saved\n",
      "iteration: 20200 \tloss: 0.004151277244091034\n",
      "model at iteration 20200 is saved\n",
      "iteration: 20300 \tloss: 0.003216454526409507\n",
      "model at iteration 20300 is saved\n",
      "iteration: 20400 \tloss: 0.0033982042223215103\n",
      "model at iteration 20400 is saved\n",
      "iteration: 20500 \tloss: 0.004911968484520912\n",
      "model at iteration 20500 is saved\n",
      "iteration: 20600 \tloss: 0.005505316890776157\n",
      "model at iteration 20600 is saved\n",
      "iteration: 20700 \tloss: 0.003906994126737118\n",
      "model at iteration 20700 is saved\n",
      "iteration: 20800 \tloss: 0.004965928848832846\n",
      "model at iteration 20800 is saved\n",
      "iteration: 20900 \tloss: 0.002588581060990691\n",
      "model at iteration 20900 is saved\n",
      "iteration: 21000 \tloss: 0.0034346282482147217\n",
      "model at iteration 21000 is saved\n",
      "iteration: 21100 \tloss: 0.0077470438554883\n",
      "model at iteration 21100 is saved\n",
      "iteration: 21200 \tloss: 0.007834442891180515\n",
      "model at iteration 21200 is saved\n",
      "iteration: 21300 \tloss: 0.004273995291441679\n",
      "model at iteration 21300 is saved\n",
      "iteration: 21400 \tloss: 0.004048471804708242\n",
      "model at iteration 21400 is saved\n",
      "iteration: 21500 \tloss: 0.005292185116559267\n",
      "model at iteration 21500 is saved\n",
      "iteration: 21600 \tloss: 0.007563869934529066\n",
      "model at iteration 21600 is saved\n",
      "iteration: 21700 \tloss: 0.003943122923374176\n",
      "model at iteration 21700 is saved\n",
      "iteration: 21800 \tloss: 0.004201785661280155\n",
      "model at iteration 21800 is saved\n",
      "iteration: 21900 \tloss: 0.0029379157349467278\n",
      "model at iteration 21900 is saved\n",
      "iteration: 22000 \tloss: 0.0025758894626051188\n",
      "model at iteration 22000 is saved\n",
      "iteration: 22100 \tloss: 0.004586865194141865\n",
      "model at iteration 22100 is saved\n",
      "iteration: 22200 \tloss: 0.0045389579609036446\n",
      "model at iteration 22200 is saved\n",
      "iteration: 22300 \tloss: 0.004637465812265873\n",
      "model at iteration 22300 is saved\n",
      "iteration: 22400 \tloss: 0.007586903404444456\n",
      "model at iteration 22400 is saved\n",
      "iteration: 22500 \tloss: 0.0038632552605122328\n",
      "model at iteration 22500 is saved\n",
      "iteration: 22600 \tloss: 0.002497500739991665\n",
      "model at iteration 22600 is saved\n",
      "iteration: 22700 \tloss: 0.0035371119156479836\n",
      "model at iteration 22700 is saved\n",
      "iteration: 22800 \tloss: 0.002676713978871703\n",
      "model at iteration 22800 is saved\n",
      "iteration: 22900 \tloss: 0.003911728039383888\n",
      "model at iteration 22900 is saved\n",
      "iteration: 23000 \tloss: 0.0043891496025025845\n",
      "model at iteration 23000 is saved\n",
      "iteration: 23100 \tloss: 0.006321350112557411\n",
      "model at iteration 23100 is saved\n",
      "iteration: 23200 \tloss: 0.0044577037915587425\n",
      "model at iteration 23200 is saved\n",
      "iteration: 23300 \tloss: 0.00342587404884398\n",
      "model at iteration 23300 is saved\n",
      "iteration: 23400 \tloss: 0.003732329001650214\n",
      "model at iteration 23400 is saved\n",
      "iteration: 23500 \tloss: 0.003213064279407263\n",
      "model at iteration 23500 is saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m X \u001b[38;5;241m=\u001b[39m batch, batch, mask, loss_mask\n\u001b[0;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_hyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                        \u001b[49m\u001b[43monly_generate_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_generate_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/mnt/sdb/hanyuji-workbench/SSSD/utils/util.py:225\u001b[0m, in \u001b[0;36mtraining_loss\u001b[0;34m(net, loss_fn, X, diffusion_hyperparams, only_generate_missing)\u001b[0m\n\u001b[1;32m    220\u001b[0m     z \u001b[38;5;241m=\u001b[39m audio \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m+\u001b[39m z \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    221\u001b[0m transformed_X \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    222\u001b[0m     torch\u001b[38;5;241m.\u001b[39msqrt(Alpha_bar[diffusion_steps]) \u001b[38;5;241m*\u001b[39m audio\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Alpha_bar[diffusion_steps]) \u001b[38;5;241m*\u001b[39m z\n\u001b[1;32m    224\u001b[0m )  \u001b[38;5;66;03m# compute x_t from q(x_t|x_0)\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m epsilon_theta \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformed_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# predict \\epsilon according to \\epsilon_\\theta\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m only_generate_missing \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_fn(epsilon_theta[loss_mask], z[loss_mask])\n",
      "File \u001b[0;32m~/miniconda3/envs/SSSD/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb/hanyuji-workbench/SSSD/imputers/SSSDS4Imputer.py:194\u001b[0m, in \u001b[0;36mSSSDS4Imputer.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    192\u001b[0m x \u001b[38;5;241m=\u001b[39m noise\n\u001b[1;32m    193\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_conv(x)\n\u001b[0;32m--> 194\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(x)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/SSSD/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb/hanyuji-workbench/SSSD/imputers/SSSDS4Imputer.py:147\u001b[0m, in \u001b[0;36mResidual_group.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    145\u001b[0m skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_res_layers):\n\u001b[0;32m--> 147\u001b[0m     h, skip_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_step_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    148\u001b[0m     skip \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m skip_n  \n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m skip \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_res_layers)\n",
      "File \u001b[0;32m~/miniconda3/envs/SSSD/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb/hanyuji-workbench/SSSD/imputers/SSSDS4Imputer.py:96\u001b[0m, in \u001b[0;36mResidual_block.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     93\u001b[0m cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_conv(cond)\n\u001b[1;32m     94\u001b[0m h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cond\n\u001b[0;32m---> 96\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS42\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     98\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(h[:,:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_channels,:]) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(h[:,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_channels:,:])\n\u001b[1;32m    100\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_conv(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/SSSD/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb/hanyuji-workbench/SSSD/imputers/S4Model.py:1198\u001b[0m, in \u001b[0;36mS4Layer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;66;03m#x has shape seq, batch, feature\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m#batch, feature, seq (as expected from S4 with transposed=True)\u001b[39;00m\n\u001b[0;32m-> 1198\u001b[0m     xout, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms4_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#batch, feature, seq\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m     xout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(xout)\n\u001b[1;32m   1200\u001b[0m     xout \u001b[38;5;241m=\u001b[39m xout \u001b[38;5;241m+\u001b[39m x \u001b[38;5;66;03m# skip connection   # batch, feature, seq\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/SSSD/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb/hanyuji-workbench/SSSD/imputers/S4Model.py:1109\u001b[0m, in \u001b[0;36mS4.forward\u001b[0;34m(self, u, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m L \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;66;03m# Compute SS Kernel\u001b[39;00m\n\u001b[0;32m-> 1109\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (C H L) (B C H L)\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Convolution\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional:\n",
      "File \u001b[0;32m~/miniconda3/envs/SSSD/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb/hanyuji-workbench/SSSD/imputers/S4Model.py:988\u001b[0m, in \u001b[0;36mHippoSSKernel.forward\u001b[0;34m(self, L)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, L\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 988\u001b[0m     k, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m k\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/miniconda3/envs/SSSD/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb/hanyuji-workbench/SSSD/imputers/S4Model.py:642\u001b[0m, in \u001b[0;36mSSKernelNPLR.forward\u001b[0;34m(self, state, rate, L)\u001b[0m\n\u001b[1;32m    638\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_w()\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# Use cached FFT nodes\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     omega, z \u001b[38;5;241m=\u001b[39m \u001b[43m_r2c\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m, _r2c(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz)  \u001b[38;5;66;03m# (..., L)\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     omega, z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_omega(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL\u001b[38;5;241m/\u001b[39mrate), dtype\u001b[38;5;241m=\u001b[39mw\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mw\u001b[38;5;241m.\u001b[39mdevice, cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "n_iter = ckpt_iter + 1\n",
    "while n_iter < n_iters + 1:\n",
    "    for batch in training_data:\n",
    "\n",
    "        if masking == 'rm':\n",
    "            transposed_mask = get_mask_rm(batch[0], missing_k)\n",
    "        elif masking == 'mnr':\n",
    "            transposed_mask = get_mask_mnr(batch[0], missing_k)\n",
    "        elif masking == 'bm':\n",
    "            transposed_mask = get_mask_bm(batch[0], missing_k)\n",
    "\n",
    "        mask = transposed_mask.permute(1, 0)\n",
    "        mask = mask.repeat(batch.size()[0], 1, 1).float().cuda()\n",
    "        loss_mask = ~mask.bool()\n",
    "        batch = batch.permute(0, 2, 1)\n",
    "\n",
    "        assert batch.size() == mask.size() == loss_mask.size()\n",
    "\n",
    "        # back-propagation\n",
    "        optimizer.zero_grad()\n",
    "        X = batch, batch, mask, loss_mask\n",
    "        loss = training_loss(net, nn.MSELoss(), X, diffusion_hyperparams,\n",
    "                                only_generate_missing=only_generate_missing)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if n_iter % iters_per_logging == 0:\n",
    "            print(\"iteration: {} \\tloss: {}\".format(n_iter, loss.item()))\n",
    "\n",
    "        # save checkpoint\n",
    "        if n_iter > 0 and n_iter % iters_per_ckpt == 0:\n",
    "            checkpoint_name = '{}.pkl'.format(n_iter)\n",
    "            torch.save({'model_state_dict': net.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict()},\n",
    "                        os.path.join(output_directory, checkpoint_name))\n",
    "            print('model at iteration %s is saved' % n_iter)\n",
    "\n",
    "        n_iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSSD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
